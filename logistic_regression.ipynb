{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import tqdm\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot  as plt\n",
    "import os\n",
    "import numpy as np\n",
    "from sklearn import linear_model, decomposition, datasets\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import random \n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import cross_val_score\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_SIZE = \"xs\"\n",
    "DATASET_TYPE = \"waveform\"\n",
    "\n",
    "DATASET_FOLDER = f\"./data/{DATASET_TYPE}\"\n",
    "\n",
    "DATASET_NUM_SAMPLES_PER_SECOND = 8000\n",
    "DATASET_NUM_CHANNELS = 1\n",
    "\n",
    "DATASET_NAME = f\"fma_{DATASET_SIZE}_resampled_{DATASET_NUM_SAMPLES_PER_SECOND}_rechanneled_{DATASET_NUM_CHANNELS}\"\n",
    "\n",
    "dataset_path = f\"{DATASET_FOLDER}/{DATASET_NAME}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_path_list = []\n",
    "\n",
    "for path, subdirs, files in os.walk(dataset_path):\n",
    "    for name in files:\n",
    "        file_audio_path = os.path.join(path, name)\n",
    "        print(file_audio_path)\n",
    "\n",
    "        if name != '.DS_Store':\n",
    "            audio_path_list.append(file_audio_path)\n",
    "\n",
    "audio_path_list = sorted(audio_path_list , reverse= True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "single_tensors = []\n",
    "labels = []\n",
    "for p in audio_path_list:\n",
    "    single_tensors.append(torch.load(p))\n",
    "    labels.append(p.split(\"/\")[-2])\n",
    "\n",
    "\n",
    "stacked_single_tensors = torch.cat(single_tensors).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.DataFrame(stacked_single_tensors)\n",
    "data.insert(0, \"Label\", labels, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_split( dataset , perc_train , set_seed = 69):\n",
    "    train = dataset.sample(frac= perc_train,random_state = set_seed )\n",
    "    test  = dataset.drop(train.index)\n",
    "    return (train , test )\n",
    "#####   \n",
    "train_dataset , test_dataset = train_test_split( data , perc_train = .85)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = train_dataset.iloc[: , 1:]      # ALL THE DATA POINTS\n",
    "y = train_dataset.iloc[: , :1]      # ALL THE LABELS\n",
    "\n",
    "std_slc = StandardScaler()          # STANDARDIZE\n",
    "\n",
    "pca = decomposition.PCA()           # PCA\n",
    "\n",
    "logistic_Reg = linear_model.LogisticRegression()    # LINEAR REGRESSION\n",
    "\n",
    "### \n",
    "pipe = Pipeline(steps=[('std_slc', std_slc),\n",
    "                ('pca', pca),\n",
    "                ('logistic_Reg', logistic_Reg)])\n",
    "\n",
    "\n",
    "n_components = list(range( int(.01 * X.shape[0]) ,  int(.25 * X.shape[0]) , 1))    # NUMBER OF PCA WE TEST\n",
    "# WE TRIED DIFFERENTS COMBINATIONS AND WE ENDED UP DISCOVERING THAT FEW PRINCIPAL COMPONENTS WERE ENOUGH\n",
    "# WE NOW TEST ONLY ON THE FIRST 25% OF THE POSSIBLE NUMBER OF PC's.\n",
    "\n",
    "C = [2., 20.]      # NUMBER OF C VALUE WE TEST\n",
    "\n",
    "penalty = ['l1', 'l2']    # PENALTY WE TEST\n",
    "\n",
    "# COMBINE ALL IN A DICTIONARY\n",
    "parameters = dict(pca__n_components=n_components,\n",
    "                    logistic_Reg__C=C,\n",
    "                    logistic_Reg__penalty=penalty)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### RUN THE KFOLD CROSS VALIDATION TO DISCOVER THE BEST HYPERPARAMETERS\n",
    "clf = GridSearchCV(pipe, parameters, n_jobs=2, verbose=3, cv=3)\n",
    "clf.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Penalty: l2\n",
      "Best C: 2.0\n",
      "Best Number Of Components: 52\n",
      "\n",
      "LogisticRegression(C=2.0)\n"
     ]
    }
   ],
   "source": [
    "print('Best Penalty:', clf.best_estimator_.get_params()['logistic_Reg__penalty'])\n",
    "print('Best C:', clf.best_estimator_.get_params()['logistic_Reg__C'])\n",
    "print('Best Number Of Components:', clf.best_estimator_.get_params()['pca__n_components'])\n",
    "print(); print(clf.best_estimator_.get_params()['logistic_Reg'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# WE DON'T KNOW WHY IS HERE\n",
    "#CV_result = cross_val_score(clf , X , y , cv = 3 , n_jobs=2, verbose=3)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the Logistcics regression with the best hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = train_dataset.iloc[: , 1:]      # ALL THE DATA POINTS for the training set \n",
    "y_train = train_dataset.iloc[: , :1]      # ALL THE LABELSfor the training set \n",
    "\n",
    "X_test =  test_dataset.iloc[: , 1:]       # ALL THE DATA POINTS for the test set \n",
    "y_test =  test_dataset.iloc[: , :1]       # ALL THE LABELS for the test set\n",
    "\n",
    "# [TODO]  do not arcoding but with the best hyperparameter\n",
    "\n",
    "pca = PCA(n_components= 11)               # PCA with the best number of components\n",
    "X_train_pca =  pd.DataFrame(pca.fit_transform(X_train))   # Transform the train dataset in pc\n",
    "X_test_pca  =  pd.DataFrame(pca.fit_transform(X_test ))   # Transform the test  dataset in pc\n",
    "\n",
    "### RUN THE LOGISTIC REGRESSION\n",
    "\n",
    "logistic = LogisticRegression(penalty= 'l2'  , C = 2)\n",
    "logistic.fit( X_train_pca  , y_train )\n",
    "\n",
    "### TEST THE MODEL\n",
    "LogisticPredictions = logistic.predict( X_test_pca )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy of the logistic model is: 21%\n"
     ]
    }
   ],
   "source": [
    "Logistic_Accuracy = logistic.score(  X_test_pca , y_test ) \n",
    "Logistic_Accuracy =\"{: .0%}\".format(Logistic_Accuracy)\n",
    "print(f\"The accuracy of the logistic model is:{Logistic_Accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 864x720 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "### CONFUSION MATRIX\n",
    "confusion = metrics.confusion_matrix( y_test , LogisticPredictions)\n",
    "plt.figure(figsize=(12,10))\n",
    "sns.heatmap(confusion , annot= True , fmt = 'd' , cmap= \"viridis\")\n",
    "plt.xlabel(\"Recognized Genres\")\n",
    "plt.ylabel(\"Actual Genre\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    " \n",
    "print('Shape before PCA: ', X.shape)\n",
    "print('Shape after PCA: ', pca_features.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt \n",
    "\n",
    "plt.rcParams[\"figure.figsize\"] = (12,8)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "xi = np.arange(1, 61, step=1)\n",
    "y = np.cumsum(pca.explained_variance_ratio_)\n",
    "\n",
    "plt.ylim(0.0,1.1)\n",
    "plt.plot(xi, y, marker='o', linestyle='-', color='red')\n",
    "\n",
    "plt.xlabel('Number of Components')\n",
    "plt.xticks(np.arange(1, 61, step=1), rotation = 90) \n",
    "plt.ylabel('Cumulative variance (%)')\n",
    "plt.title('The number of components needed to explain variance')\n",
    "\n",
    "plt.axhline(y=0.7, color='darkgreen', linestyle='--')\n",
    "plt.text(1.1, 1, '70% cut-off threshold', color = 'darkgreen', fontsize=16)\n",
    "\n",
    "ax.grid()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>237990</th>\n",
       "      <th>237991</th>\n",
       "      <th>237992</th>\n",
       "      <th>237993</th>\n",
       "      <th>237994</th>\n",
       "      <th>237995</th>\n",
       "      <th>237996</th>\n",
       "      <th>237997</th>\n",
       "      <th>237998</th>\n",
       "      <th>237999</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.008234</td>\n",
       "      <td>0.010684</td>\n",
       "      <td>0.003508</td>\n",
       "      <td>0.002686</td>\n",
       "      <td>-0.001900</td>\n",
       "      <td>0.001159</td>\n",
       "      <td>-0.000374</td>\n",
       "      <td>-0.003358</td>\n",
       "      <td>0.003079</td>\n",
       "      <td>-0.001310</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.203883</td>\n",
       "      <td>-0.437538</td>\n",
       "      <td>-0.513192</td>\n",
       "      <td>-0.509520</td>\n",
       "      <td>-0.464054</td>\n",
       "      <td>-0.262243</td>\n",
       "      <td>0.100966</td>\n",
       "      <td>0.051868</td>\n",
       "      <td>0.126681</td>\n",
       "      <td>0.202008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.000242</td>\n",
       "      <td>0.000148</td>\n",
       "      <td>0.000029</td>\n",
       "      <td>0.000122</td>\n",
       "      <td>0.000174</td>\n",
       "      <td>-0.000082</td>\n",
       "      <td>-0.000124</td>\n",
       "      <td>0.000156</td>\n",
       "      <td>-0.000041</td>\n",
       "      <td>-0.000223</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.262216</td>\n",
       "      <td>-0.189099</td>\n",
       "      <td>-0.089163</td>\n",
       "      <td>-0.122821</td>\n",
       "      <td>-0.168169</td>\n",
       "      <td>-0.136992</td>\n",
       "      <td>0.008826</td>\n",
       "      <td>0.082147</td>\n",
       "      <td>0.049861</td>\n",
       "      <td>0.062775</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.004516</td>\n",
       "      <td>0.002586</td>\n",
       "      <td>-0.000504</td>\n",
       "      <td>0.002324</td>\n",
       "      <td>0.001361</td>\n",
       "      <td>0.001164</td>\n",
       "      <td>0.001756</td>\n",
       "      <td>0.002061</td>\n",
       "      <td>0.001286</td>\n",
       "      <td>-0.001631</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.154083</td>\n",
       "      <td>-0.204206</td>\n",
       "      <td>-0.137308</td>\n",
       "      <td>-0.143353</td>\n",
       "      <td>-0.134408</td>\n",
       "      <td>-0.125974</td>\n",
       "      <td>-0.023337</td>\n",
       "      <td>-0.002398</td>\n",
       "      <td>-0.081676</td>\n",
       "      <td>-0.022399</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>0.001199</td>\n",
       "      <td>0.001048</td>\n",
       "      <td>0.003401</td>\n",
       "      <td>0.001240</td>\n",
       "      <td>0.000462</td>\n",
       "      <td>0.000892</td>\n",
       "      <td>-0.000196</td>\n",
       "      <td>-0.000868</td>\n",
       "      <td>0.001279</td>\n",
       "      <td>0.001767</td>\n",
       "      <td>...</td>\n",
       "      <td>0.015631</td>\n",
       "      <td>-0.011225</td>\n",
       "      <td>-0.002610</td>\n",
       "      <td>0.003677</td>\n",
       "      <td>0.002981</td>\n",
       "      <td>-0.013040</td>\n",
       "      <td>-0.019982</td>\n",
       "      <td>0.006720</td>\n",
       "      <td>0.035207</td>\n",
       "      <td>0.087214</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>0.005291</td>\n",
       "      <td>0.004534</td>\n",
       "      <td>-0.002246</td>\n",
       "      <td>-0.006056</td>\n",
       "      <td>-0.010251</td>\n",
       "      <td>-0.008787</td>\n",
       "      <td>-0.002304</td>\n",
       "      <td>0.002260</td>\n",
       "      <td>0.001732</td>\n",
       "      <td>0.003227</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.013173</td>\n",
       "      <td>-0.997653</td>\n",
       "      <td>-0.932473</td>\n",
       "      <td>-0.875366</td>\n",
       "      <td>-0.887953</td>\n",
       "      <td>-0.819201</td>\n",
       "      <td>-0.735276</td>\n",
       "      <td>-0.657150</td>\n",
       "      <td>-0.716523</td>\n",
       "      <td>-0.684720</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>0.004482</td>\n",
       "      <td>0.003281</td>\n",
       "      <td>-0.000192</td>\n",
       "      <td>-0.002675</td>\n",
       "      <td>-0.000145</td>\n",
       "      <td>0.005437</td>\n",
       "      <td>0.002224</td>\n",
       "      <td>-0.003226</td>\n",
       "      <td>-0.003051</td>\n",
       "      <td>-0.000294</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.330977</td>\n",
       "      <td>-0.526317</td>\n",
       "      <td>-0.698106</td>\n",
       "      <td>-0.871481</td>\n",
       "      <td>-1.000397</td>\n",
       "      <td>-1.104575</td>\n",
       "      <td>-1.143645</td>\n",
       "      <td>-1.102311</td>\n",
       "      <td>-1.077322</td>\n",
       "      <td>-1.000269</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>0.006836</td>\n",
       "      <td>0.003612</td>\n",
       "      <td>0.009416</td>\n",
       "      <td>0.010575</td>\n",
       "      <td>0.010387</td>\n",
       "      <td>0.008242</td>\n",
       "      <td>0.006145</td>\n",
       "      <td>0.006070</td>\n",
       "      <td>0.004588</td>\n",
       "      <td>0.004461</td>\n",
       "      <td>...</td>\n",
       "      <td>0.437255</td>\n",
       "      <td>0.558165</td>\n",
       "      <td>0.561081</td>\n",
       "      <td>0.572451</td>\n",
       "      <td>0.544912</td>\n",
       "      <td>0.429185</td>\n",
       "      <td>0.369773</td>\n",
       "      <td>0.284628</td>\n",
       "      <td>0.234021</td>\n",
       "      <td>0.208999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>-0.000080</td>\n",
       "      <td>-0.000053</td>\n",
       "      <td>-0.000033</td>\n",
       "      <td>0.000016</td>\n",
       "      <td>0.000055</td>\n",
       "      <td>0.000064</td>\n",
       "      <td>-0.000012</td>\n",
       "      <td>-0.000041</td>\n",
       "      <td>-0.000007</td>\n",
       "      <td>-0.000029</td>\n",
       "      <td>...</td>\n",
       "      <td>0.013216</td>\n",
       "      <td>0.006755</td>\n",
       "      <td>0.002225</td>\n",
       "      <td>0.000341</td>\n",
       "      <td>-0.000879</td>\n",
       "      <td>-0.002471</td>\n",
       "      <td>-0.007369</td>\n",
       "      <td>-0.014305</td>\n",
       "      <td>-0.018592</td>\n",
       "      <td>-0.019696</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>0.017488</td>\n",
       "      <td>-0.009679</td>\n",
       "      <td>-0.018584</td>\n",
       "      <td>-0.017172</td>\n",
       "      <td>-0.025965</td>\n",
       "      <td>-0.002325</td>\n",
       "      <td>-0.012070</td>\n",
       "      <td>-0.021997</td>\n",
       "      <td>0.048340</td>\n",
       "      <td>0.060432</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.033159</td>\n",
       "      <td>0.018963</td>\n",
       "      <td>0.024293</td>\n",
       "      <td>-0.019560</td>\n",
       "      <td>0.004781</td>\n",
       "      <td>-0.004360</td>\n",
       "      <td>0.031849</td>\n",
       "      <td>0.034502</td>\n",
       "      <td>-0.027264</td>\n",
       "      <td>0.014638</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>9 rows Ã— 238000 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      0         1         2         3         4         5         6       \\\n",
       "9   0.008234  0.010684  0.003508  0.002686 -0.001900  0.001159 -0.000374   \n",
       "20  0.000242  0.000148  0.000029  0.000122  0.000174 -0.000082 -0.000124   \n",
       "22  0.004516  0.002586 -0.000504  0.002324  0.001361  0.001164  0.001756   \n",
       "49  0.001199  0.001048  0.003401  0.001240  0.000462  0.000892 -0.000196   \n",
       "54  0.005291  0.004534 -0.002246 -0.006056 -0.010251 -0.008787 -0.002304   \n",
       "55  0.004482  0.003281 -0.000192 -0.002675 -0.000145  0.005437  0.002224   \n",
       "56  0.006836  0.003612  0.009416  0.010575  0.010387  0.008242  0.006145   \n",
       "73 -0.000080 -0.000053 -0.000033  0.000016  0.000055  0.000064 -0.000012   \n",
       "75  0.017488 -0.009679 -0.018584 -0.017172 -0.025965 -0.002325 -0.012070   \n",
       "\n",
       "      7         8         9       ...    237990    237991    237992    237993  \\\n",
       "9  -0.003358  0.003079 -0.001310  ... -0.203883 -0.437538 -0.513192 -0.509520   \n",
       "20  0.000156 -0.000041 -0.000223  ... -0.262216 -0.189099 -0.089163 -0.122821   \n",
       "22  0.002061  0.001286 -0.001631  ... -0.154083 -0.204206 -0.137308 -0.143353   \n",
       "49 -0.000868  0.001279  0.001767  ...  0.015631 -0.011225 -0.002610  0.003677   \n",
       "54  0.002260  0.001732  0.003227  ... -1.013173 -0.997653 -0.932473 -0.875366   \n",
       "55 -0.003226 -0.003051 -0.000294  ... -0.330977 -0.526317 -0.698106 -0.871481   \n",
       "56  0.006070  0.004588  0.004461  ...  0.437255  0.558165  0.561081  0.572451   \n",
       "73 -0.000041 -0.000007 -0.000029  ...  0.013216  0.006755  0.002225  0.000341   \n",
       "75 -0.021997  0.048340  0.060432  ... -0.033159  0.018963  0.024293 -0.019560   \n",
       "\n",
       "      237994    237995    237996    237997    237998    237999  \n",
       "9  -0.464054 -0.262243  0.100966  0.051868  0.126681  0.202008  \n",
       "20 -0.168169 -0.136992  0.008826  0.082147  0.049861  0.062775  \n",
       "22 -0.134408 -0.125974 -0.023337 -0.002398 -0.081676 -0.022399  \n",
       "49  0.002981 -0.013040 -0.019982  0.006720  0.035207  0.087214  \n",
       "54 -0.887953 -0.819201 -0.735276 -0.657150 -0.716523 -0.684720  \n",
       "55 -1.000397 -1.104575 -1.143645 -1.102311 -1.077322 -1.000269  \n",
       "56  0.544912  0.429185  0.369773  0.284628  0.234021  0.208999  \n",
       "73 -0.000879 -0.002471 -0.007369 -0.014305 -0.018592 -0.019696  \n",
       "75  0.004781 -0.004360  0.031849  0.034502 -0.027264  0.014638  \n",
       "\n",
       "[9 rows x 238000 columns]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dataset.iloc[: , 1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components= 7  )\n",
    "dataset_pca = pca.fit_transform(train_dataset.iloc[: , 1:])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "label = train_dataset.iloc[ : , :1 ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_train =[]\n",
    "for x in label.values:\n",
    "    labels_train.append(_label_from_str_to_one_hot(x[0]))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_pca = pd.DataFrame(dataset_pca)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_test = PCA(n_components= 52 )\n",
    "test_dataset_pca = pca.fit_transform(test_dataset.iloc[: , 1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "lab_test = pd.get_dummies(test_dataset.iloc[ : , :1 ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Label_Electronic</th>\n",
       "      <th>Label_Folk</th>\n",
       "      <th>Label_Hip-Hop</th>\n",
       "      <th>Label_Pop</th>\n",
       "      <th>Label_Rock</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Label_Electronic  Label_Folk  Label_Hip-Hop  Label_Pop  Label_Rock\n",
       "9                  0           0              0          0           1\n",
       "20                 0           0              0          1           0\n",
       "22                 0           0              0          1           0\n",
       "49                 0           0              1          0           0\n",
       "54                 0           0              1          0           0\n",
       "55                 0           0              1          0           0\n",
       "56                 0           0              1          0           0\n",
       "73                 0           1              0          0           0\n",
       "75                 1           0              0          0           0"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lab_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Electronic', 'Jazz', 'Jazz', 'Hip-Hop', 'Hip-Hop', 'Electronic',\n",
       "       'Jazz', 'Jazz', 'Rock'], dtype=object)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logistic.predict(test_dataset_pca)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _label_from_str_to_one_hot(label_str: str): \n",
    "\n",
    "    if label_str == \"Pop\":\n",
    "        return [1, 0, 0, 0, 0, 0]\n",
    "    \n",
    "    if label_str == \"Hip-Hop\":\n",
    "        return [0, 1, 0, 0, 0, 0]\n",
    "    \n",
    "    if label_str == \"Electronic\":\n",
    "        return [0, 0, 1, 0, 0, 0]\n",
    "    \n",
    "    if label_str == \"Rock\":\n",
    "        return [0, 0, 0, 1, 0, 0]\n",
    "\n",
    "    if label_str == \"Folk\":\n",
    "        return [0, 0, 0, 0, 1, 0]\n",
    "\n",
    "    if label_str == \"Jazz\":\n",
    "        return [0, 0, 0, 0, 0, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0, 0, 1, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 1],\n",
       " [0, 0, 0, 0, 0, 1],\n",
       " [0, 1, 0, 0, 0, 0],\n",
       " [0, 1, 0, 0, 0, 0],\n",
       " [0, 0, 1, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 1],\n",
       " [0, 0, 0, 0, 0, 1],\n",
       " [0, 0, 0, 1, 0, 0]]"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "X has 6 features, but LogisticRegression is expecting 7 features as input.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/Users/paolo/Desktop/GeNNus/logistic_regression.ipynb Cella 32\u001b[0m in \u001b[0;36m<cell line: 11>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/paolo/Desktop/GeNNus/logistic_regression.ipynb#X25sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m label_test\u001b[39m.\u001b[39mvalues:\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/paolo/Desktop/GeNNus/logistic_regression.ipynb#X25sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m     lab_test\u001b[39m.\u001b[39mappend(_label_from_str_to_one_hot(x[\u001b[39m0\u001b[39m]))\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/paolo/Desktop/GeNNus/logistic_regression.ipynb#X25sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m Logistic_Accuracy \u001b[39m=\u001b[39m logistic\u001b[39m.\u001b[39;49mscore(  results , lab_test )\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/sklearn/base.py:651\u001b[0m, in \u001b[0;36mClassifierMixin.score\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    626\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    627\u001b[0m \u001b[39mReturn the mean accuracy on the given test data and labels.\u001b[39;00m\n\u001b[1;32m    628\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    647\u001b[0m \u001b[39m    Mean accuracy of ``self.predict(X)`` wrt. `y`.\u001b[39;00m\n\u001b[1;32m    648\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    649\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mmetrics\u001b[39;00m \u001b[39mimport\u001b[39;00m accuracy_score\n\u001b[0;32m--> 651\u001b[0m \u001b[39mreturn\u001b[39;00m accuracy_score(y, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpredict(X), sample_weight\u001b[39m=\u001b[39msample_weight)\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_base.py:425\u001b[0m, in \u001b[0;36mLinearClassifierMixin.predict\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    411\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mpredict\u001b[39m(\u001b[39mself\u001b[39m, X):\n\u001b[1;32m    412\u001b[0m     \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    413\u001b[0m \u001b[39m    Predict class labels for samples in X.\u001b[39;00m\n\u001b[1;32m    414\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    423\u001b[0m \u001b[39m        Vector containing the class labels for each sample.\u001b[39;00m\n\u001b[1;32m    424\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 425\u001b[0m     scores \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdecision_function(X)\n\u001b[1;32m    426\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(scores\u001b[39m.\u001b[39mshape) \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m    427\u001b[0m         indices \u001b[39m=\u001b[39m (scores \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m)\u001b[39m.\u001b[39mastype(\u001b[39mint\u001b[39m)\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_base.py:407\u001b[0m, in \u001b[0;36mLinearClassifierMixin.decision_function\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    387\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    388\u001b[0m \u001b[39mPredict confidence scores for samples.\u001b[39;00m\n\u001b[1;32m    389\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    403\u001b[0m \u001b[39m    this class would be predicted.\u001b[39;00m\n\u001b[1;32m    404\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    405\u001b[0m check_is_fitted(\u001b[39mself\u001b[39m)\n\u001b[0;32m--> 407\u001b[0m X \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_validate_data(X, accept_sparse\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mcsr\u001b[39;49m\u001b[39m\"\u001b[39;49m, reset\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)\n\u001b[1;32m    408\u001b[0m scores \u001b[39m=\u001b[39m safe_sparse_dot(X, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcoef_\u001b[39m.\u001b[39mT, dense_output\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m) \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mintercept_\n\u001b[1;32m    409\u001b[0m \u001b[39mreturn\u001b[39;00m scores\u001b[39m.\u001b[39mravel() \u001b[39mif\u001b[39;00m scores\u001b[39m.\u001b[39mshape[\u001b[39m1\u001b[39m] \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m \u001b[39melse\u001b[39;00m scores\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/sklearn/base.py:585\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[0;34m(self, X, y, reset, validate_separately, **check_params)\u001b[0m\n\u001b[1;32m    582\u001b[0m     out \u001b[39m=\u001b[39m X, y\n\u001b[1;32m    584\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m no_val_X \u001b[39mand\u001b[39;00m check_params\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mensure_2d\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mTrue\u001b[39;00m):\n\u001b[0;32m--> 585\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_check_n_features(X, reset\u001b[39m=\u001b[39;49mreset)\n\u001b[1;32m    587\u001b[0m \u001b[39mreturn\u001b[39;00m out\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/sklearn/base.py:400\u001b[0m, in \u001b[0;36mBaseEstimator._check_n_features\u001b[0;34m(self, X, reset)\u001b[0m\n\u001b[1;32m    397\u001b[0m     \u001b[39mreturn\u001b[39;00m\n\u001b[1;32m    399\u001b[0m \u001b[39mif\u001b[39;00m n_features \u001b[39m!=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_features_in_:\n\u001b[0;32m--> 400\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    401\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mX has \u001b[39m\u001b[39m{\u001b[39;00mn_features\u001b[39m}\u001b[39;00m\u001b[39m features, but \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    402\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mis expecting \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_features_in_\u001b[39m}\u001b[39;00m\u001b[39m features as input.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    403\u001b[0m     )\n",
      "\u001b[0;31mValueError\u001b[0m: X has 6 features, but LogisticRegression is expecting 7 features as input."
     ]
    }
   ],
   "source": [
    "\n",
    "LogisticPredictions = logistic.predict( test_dataset_pca )\n",
    "results = []\n",
    "for x in LogisticPredictions:\n",
    "    results.append(_label_from_str_to_one_hot(x))\n",
    "\n",
    "lab_test = []\n",
    "for x in label_test.values:\n",
    "    lab_test.append(_label_from_str_to_one_hot(x[0]))\n",
    "\n",
    "\n",
    "Logistic_Accuracy = logistic.score(  results , lab_test ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Electronic', 'Jazz', 'Jazz', 'Hip-Hop', 'Hip-Hop', 'Electronic',\n",
       "       'Jazz', 'Jazz', 'Rock'], dtype=object)"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LogisticPredictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "lab_test = []\n",
    "for x in label_test.values:\n",
    "    lab_test.append(x[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Rock',\n",
       " 'Pop',\n",
       " 'Pop',\n",
       " 'Hip-Hop',\n",
       " 'Hip-Hop',\n",
       " 'Hip-Hop',\n",
       " 'Hip-Hop',\n",
       " 'Folk',\n",
       " 'Electronic']"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lab_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "could not convert string to float: 'Electronic'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/Users/paolo/Desktop/GeNNus/logistic_regression.ipynb Cella 29\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/paolo/Desktop/GeNNus/logistic_regression.ipynb#X26sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m LogisticPredictions \u001b[39m=\u001b[39m logistic\u001b[39m.\u001b[39mpredict( test_dataset_pca )\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/paolo/Desktop/GeNNus/logistic_regression.ipynb#X26sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m Logistic_Accuracy \u001b[39m=\u001b[39m logistic\u001b[39m.\u001b[39;49mscore(  LogisticPredictions , lab_test )\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/sklearn/base.py:651\u001b[0m, in \u001b[0;36mClassifierMixin.score\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    626\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    627\u001b[0m \u001b[39mReturn the mean accuracy on the given test data and labels.\u001b[39;00m\n\u001b[1;32m    628\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    647\u001b[0m \u001b[39m    Mean accuracy of ``self.predict(X)`` wrt. `y`.\u001b[39;00m\n\u001b[1;32m    648\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    649\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mmetrics\u001b[39;00m \u001b[39mimport\u001b[39;00m accuracy_score\n\u001b[0;32m--> 651\u001b[0m \u001b[39mreturn\u001b[39;00m accuracy_score(y, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpredict(X), sample_weight\u001b[39m=\u001b[39msample_weight)\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_base.py:425\u001b[0m, in \u001b[0;36mLinearClassifierMixin.predict\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    411\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mpredict\u001b[39m(\u001b[39mself\u001b[39m, X):\n\u001b[1;32m    412\u001b[0m     \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    413\u001b[0m \u001b[39m    Predict class labels for samples in X.\u001b[39;00m\n\u001b[1;32m    414\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    423\u001b[0m \u001b[39m        Vector containing the class labels for each sample.\u001b[39;00m\n\u001b[1;32m    424\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 425\u001b[0m     scores \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdecision_function(X)\n\u001b[1;32m    426\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(scores\u001b[39m.\u001b[39mshape) \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m    427\u001b[0m         indices \u001b[39m=\u001b[39m (scores \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m)\u001b[39m.\u001b[39mastype(\u001b[39mint\u001b[39m)\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_base.py:407\u001b[0m, in \u001b[0;36mLinearClassifierMixin.decision_function\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    387\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    388\u001b[0m \u001b[39mPredict confidence scores for samples.\u001b[39;00m\n\u001b[1;32m    389\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    403\u001b[0m \u001b[39m    this class would be predicted.\u001b[39;00m\n\u001b[1;32m    404\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    405\u001b[0m check_is_fitted(\u001b[39mself\u001b[39m)\n\u001b[0;32m--> 407\u001b[0m X \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_validate_data(X, accept_sparse\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mcsr\u001b[39;49m\u001b[39m\"\u001b[39;49m, reset\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)\n\u001b[1;32m    408\u001b[0m scores \u001b[39m=\u001b[39m safe_sparse_dot(X, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcoef_\u001b[39m.\u001b[39mT, dense_output\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m) \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mintercept_\n\u001b[1;32m    409\u001b[0m \u001b[39mreturn\u001b[39;00m scores\u001b[39m.\u001b[39mravel() \u001b[39mif\u001b[39;00m scores\u001b[39m.\u001b[39mshape[\u001b[39m1\u001b[39m] \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m \u001b[39melse\u001b[39;00m scores\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/sklearn/base.py:566\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[0;34m(self, X, y, reset, validate_separately, **check_params)\u001b[0m\n\u001b[1;32m    564\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mValidation should be done on X, y or both.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    565\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mnot\u001b[39;00m no_val_X \u001b[39mand\u001b[39;00m no_val_y:\n\u001b[0;32m--> 566\u001b[0m     X \u001b[39m=\u001b[39m check_array(X, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mcheck_params)\n\u001b[1;32m    567\u001b[0m     out \u001b[39m=\u001b[39m X\n\u001b[1;32m    568\u001b[0m \u001b[39melif\u001b[39;00m no_val_X \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m no_val_y:\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/sklearn/utils/validation.py:746\u001b[0m, in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator)\u001b[0m\n\u001b[1;32m    744\u001b[0m         array \u001b[39m=\u001b[39m array\u001b[39m.\u001b[39mastype(dtype, casting\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39munsafe\u001b[39m\u001b[39m\"\u001b[39m, copy\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[1;32m    745\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 746\u001b[0m         array \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39;49masarray(array, order\u001b[39m=\u001b[39;49morder, dtype\u001b[39m=\u001b[39;49mdtype)\n\u001b[1;32m    747\u001b[0m \u001b[39mexcept\u001b[39;00m ComplexWarning \u001b[39mas\u001b[39;00m complex_warning:\n\u001b[1;32m    748\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    749\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mComplex data not supported\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m{}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(array)\n\u001b[1;32m    750\u001b[0m     ) \u001b[39mfrom\u001b[39;00m \u001b[39mcomplex_warning\u001b[39;00m\n",
      "\u001b[0;31mValueError\u001b[0m: could not convert string to float: 'Electronic'"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Logistic_Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "confusion_matrix( Y_test , LogisticPredictions , labels = None, sample_weight=None, normalize=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "key_fold = KFold( n_splits=4 ) \n",
    "key_fold.get_n_splits(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.DataFrame(stacked_single_tensors)\n",
    "X = df.iloc[:,:-1]\n",
    "y = labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing required libraries\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import KFold \n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn import preprocessing\n",
    "from sklearn import utils\n",
    "from tqdm import tqdm\n",
    "set.seed(69)\n",
    "\n",
    "\n",
    "X = df.iloc[:,:-1]\n",
    "\n",
    "#convert y values to categorical values\n",
    "\n",
    "y = pd.DataFrame(labels)\n",
    "\n",
    "#Implementing cross validation\n",
    "\n",
    "k = 3\n",
    "kf = KFold(n_splits=k, random_state= None , shuffle= True )\n",
    "model = LogisticRegression()\n",
    "\n",
    "acc_score = []\n",
    "\n",
    "for train_index , test_index in tqdm( kf.split(X) ):\n",
    "\n",
    "    X_train , X_test = X.iloc[train_index,:],X.iloc[test_index,:]\n",
    "    y_train , y_test = y.iloc[train_index] , y.iloc[test_index]\n",
    "\n",
    "\n",
    "    model.fit(X_train,y_train)\n",
    "    pred_values = model.predict(X_test)\n",
    "\n",
    "    acc = accuracy_score( pred_values , y_test)\n",
    "    acc_score.append(acc)\n",
    "\n",
    "avg_acc_score = sum(acc_score)/k\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f4667d31aad76a2fc6367a343b03ad539ee0fdcfaae0637945dddaa0789b3d6c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
